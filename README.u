-- Main README for the Unison Carbon Pipeline project

README : Doc
README = {{
  # Unison Carbon Pipeline

  A distributed data pipeline built in [Unison](https://www.unison-lang.org/) for processing grid carbon intensity data. This project demonstrates how functional, content-addressed, distributed computing can enable carbon-aware data processing.

  ## Overview

  The Unison Carbon Pipeline processes real-time and historical carbon intensity data from the [Electricity Maps API](https://www.electricitymaps.com/) to analyze grid emissions patterns. The goal is to build a pipeline that not only processes sustainability data but is itself sustainable by scheduling computation during low-carbon periods.

  ### Key Features

  * **Type-safe JSON parsing** with Unison's decoder combinators
  * **Carbon intensity analysis** including averages, min/max, and threshold filtering
  * **REPL-driven development** for interactive exploration
  * **Functional data processing** with pure, composable functions
  * **Carbon-aware computing patterns** (roadmap)

  ## Project Status

  **Phase 1: Complete ✅**

  * ✅ JSON parsing and decoding
  * ✅ Core data types for carbon intensity records
  * ✅ Aggregation functions (average, min, max, filtering)
  * ✅ Integration with Electricity Maps API data
  * ✅ REPL-based testing and validation

  **Upcoming Phases:**
  * Phase 2: Pipeline composition and Apache Iceberg integration
  * Phase 3: Distributed processing
  * Phase 4: Carbon-aware optimization and scheduling

  ## Getting Started

  ### Running the Project

  Start UCM and explore available functions. You can search for carbon intensity functions and aggregation functions using the find command.

  Run the aggregation tests with sample data by executing testAggregations.

  Test JSON decoding by running testCleanDecoder.

  ## Core Components

  ### Data Types

  The CarbonIntensityRecord type contains fields for zone (Text), carbonIntensity (Nat), datetime (Text), isEstimated (Boolean), and estimationMethod (Optional Text).

  ### Key Functions

  * `averageCarbonIntensity` - Calculate mean carbon intensity across readings
  * `minCarbonIntensity` / `maxCarbonIntensity` - Find optimal and worst times
  * `filterLowCarbon` - Identify low-carbon time windows
  * `percentageLowCarbon` - Calculate percentage of readings below threshold

  ## Roadmap

  ### Phase 2: Pipeline Composition
  * Compose parsing → cleaning → aggregation pipeline
  * Add error handling and validation
  * Write output to Apache Iceberg tables
  * Implement time-travel queries

  ### Phase 3: Distribution
  * Implement distributed processing across nodes
  * Map-reduce style work distribution
  * Fault tolerance and partial failure handling
  * Performance comparison: local vs distributed

  ### Phase 4: Carbon-Aware Optimization
  * Real-time grid carbon intensity integration
  * Schedule heavy computation during low-carbon periods
  * Optimize for data locality
  * Measure and report pipeline carbon footprint

  ## Why Unison?

  This project leverages Unison's unique features:

  * **Content-addressed code** - Only rerun computations when code actually changes
  * **Distributed execution** - Move computation to data instead of moving data
  * **Pure functions** - Easy to test, reason about, and compose
  * **No serialization overhead** - Functions travel as code, not data
  * **REPL-driven development** - Interactive exploration and testing

  ## Resources

  ### Unison
  * [Unison Documentation](https://www.unison-lang.org/docs/)
  * [Unison Cloud](https://www.unison.cloud/)

  ### Sustainability & Carbon Data
  * [Electricity Maps API](https://www.electricitymaps.com/)
  * [Green Software Foundation](https://greensoftware.foundation/)
  * [AWS Sustainability Data Initiative](https://registry.opendata.aws/collab/asdi/)

  ---

  **Built with Unison** - Exploring the future of distributed, functional, carbon-aware computing.
}}
